{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Detailed Planning for Each Request Checkpoint\n",
    "\n",
    "This notebook handles:\n",
    "1. Loading results from Phase 1 and Phase 2\n",
    "2. For each checkpoint, generating detailed execution specifications\n",
    "3. Creating task tranches with low-level implementation tasks\n",
    "4. Defining file contexts (beginning and ending states)\n",
    "5. Saving detailed checkpoints for Phase 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shared utilities\n",
    "import sys\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "from mirascope.core import google\n",
    "from mirascope import Messages\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal\n",
    "import os\n",
    "\n",
    "from utils.interim_data_management import load_interim_data, save_interim_data\n",
    "from utils.config import get_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Previous Phase Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw_inputs from interim_data/phase1/raw_inputs.json\n",
      "Loaded files_dict from interim_data/phase1/files_dict.json\n",
      "Loaded file_analyses from interim_data/phase1/file_analyses.json\n",
      "Loaded checkpoint_outline from interim_data/phase2/checkpoint_outline.json\n",
      "Loaded data from previous phases:\n",
      "- Files: 55\n",
      "- File analyses: 55\n",
      "- Checkpoints to process: 7\n"
     ]
    }
   ],
   "source": [
    "# Get config\n",
    "config = get_config()\n",
    "\n",
    "# Load Phase 1 data\n",
    "raw_inputs = load_interim_data(config.get(\"INTERIM_DATA_DIR\"), \"raw_inputs\", \"phase1\")\n",
    "files_dict = load_interim_data(config.get(\"INTERIM_DATA_DIR\"), \"files_dict\", \"phase1\")\n",
    "file_analyses = load_interim_data(config.get(\"INTERIM_DATA_DIR\"), \"file_analyses\", \"phase1\")\n",
    "\n",
    "# Load Phase 2 data\n",
    "checkpoint_outline_data = load_interim_data(config.get(\"INTERIM_DATA_DIR\"), \"checkpoint_outline\", \"phase2\")\n",
    "\n",
    "# Extract key variables\n",
    "INPUT_NAME = raw_inputs[\"input_name\"]\n",
    "user_instructions = raw_inputs[\"user_instructions\"]\n",
    "file_tree = raw_inputs[\"file_tree\"]\n",
    "GLOBAL_CODEBASE_PURPOSE = raw_inputs[\"global_codebase_purpose\"]\n",
    "\n",
    "print(f\"Loaded data from previous phases:\")\n",
    "print(f\"- Files: {len(files_dict)}\")\n",
    "print(f\"- File analyses: {len(file_analyses)}\")\n",
    "print(f\"- Checkpoints to process: {len(checkpoint_outline_data['request_checkpoints'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct Pydantic models\n",
    "class InitialFileAnalysisItemWithContent(BaseModel):\n",
    "    file_path: str\n",
    "    overall_relevance_to_request_objective: str\n",
    "    initial_content_summary_purpose: str\n",
    "    file_contents: str\n",
    "\n",
    "class RequestCheckpointOutline(BaseModel):\n",
    "    order: int\n",
    "    title: str\n",
    "    goal_for_checkpoint: str\n",
    "    prerequisites: Optional[List[str]] = Field(default_factory=list)\n",
    "    expected_outcome: Optional[str] = None\n",
    "\n",
    "class FullRequestCheckpointOutline(BaseModel):\n",
    "    request_checkpoints: List[RequestCheckpointOutline]\n",
    "\n",
    "# Convert loaded data back to models\n",
    "all_analyses_with_content = [\n",
    "    InitialFileAnalysisItemWithContent(**item) for item in file_analyses\n",
    "]\n",
    "\n",
    "request_checkpoint_outline = FullRequestCheckpointOutline(**checkpoint_outline_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Detailed Checkpoint Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLevelTask(BaseModel):\n",
    "    \"\"\"Represents a single, atomic, low-level action to be performed.\"\"\"\n",
    "    task_id: str = Field(\n",
    "        description=\"A unique identifier for the task (e.g., '1.1.1')\"\n",
    "    )\n",
    "    file_path: str = Field(\n",
    "        description=\"The path to the file that this task will modify.\"\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"A human-readable description of what this task aims to achieve.\"\n",
    "    )\n",
    "    action_details: str = Field(\n",
    "        description=\"The specific command or instruction to be executed.\"\n",
    "    )\n",
    "    complexity: int = Field(\n",
    "        description=\"A number between 1 (trivial) and 10 (very complex) indicating the complexity.\"\n",
    "    )\n",
    "    depends_on: Optional[List[str]] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"A list of 'task_id's that must be completed before this task.\"\n",
    "    )\n",
    "  \n",
    "class TaskTranche(BaseModel):\n",
    "    \"\"\"A collection of LowLevelTasks grouped together.\"\"\"\n",
    "    tranche_id: str = Field(\n",
    "        description=\"A unique identifier for the tranche (e.g., '1.1').\"\n",
    "    )\n",
    "    goal: str = Field(\n",
    "        description=\"The specific sub-goal this tranche of tasks aims to achieve.\"\n",
    "    )\n",
    "    low_level_tasks: List[LowLevelTask] = Field(\n",
    "        description=\"An ordered list of low-level tasks to be executed.\"\n",
    "    )\n",
    "\n",
    "class RequestCheckpointExecutionSpecification(BaseModel):\n",
    "    \"\"\"The detailed plan for how to achieve a specific Request Checkpoint.\"\"\"\n",
    "    implementation_notes_specific_to_checkpoint: Optional[List[str]] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Technical details, constraints, or guidance specific to this checkpoint.\"\n",
    "    )\n",
    "    task_tranches: List[TaskTranche] = Field(\n",
    "        description=\"An ordered list of task tranches required to fulfill the checkpoint's objective.\"\n",
    "    )\n",
    "\n",
    "class RequestCheckpointFileContextItem(BaseModel):\n",
    "    \"\"\"Represents a file's state within the context of a Request Checkpoint.\"\"\"\n",
    "    file_path: str = Field(\n",
    "        description=\"The absolute or relative path to the file.\"\n",
    "    )\n",
    "    status_description: str = Field(\n",
    "        description=\"A description of the file's status.\"\n",
    "    )\n",
    "\n",
    "class RequestCheckpointSpecificFileContext(BaseModel):\n",
    "    \"\"\"Manages files relevant at the beginning and expected at the end of a checkpoint.\"\"\"\n",
    "    beginning_files_for_request_checkpoint: List[RequestCheckpointFileContextItem] = Field(\n",
    "        description=\"Files and their statuses at the start of this checkpoint.\"\n",
    "    )\n",
    "    ending_files_for_request_checkpoint: List[RequestCheckpointFileContextItem] = Field(\n",
    "        description=\"Files and their expected statuses after this checkpoint.\"\n",
    "    )\n",
    "\n",
    "class RequestCheckpoint(BaseModel):\n",
    "    \"\"\"A major, ordered milestone in the process of fulfilling the overall request.\"\"\"\n",
    "    order: int\n",
    "    title: str\n",
    "    goal_for_request_checkpoint: str\n",
    "    prerequisites: Optional[List[str]] = Field(default_factory=list)\n",
    "    expected_outcome_or_deliverables: Optional[str] = None\n",
    "    request_checkpoint_specific_file_context: RequestCheckpointSpecificFileContext\n",
    "    execution_spec: RequestCheckpointExecutionSpecification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Spec System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded spec system prompt: 67352 characters\n"
     ]
    }
   ],
   "source": [
    "def load_spec_system_prompt() -> str:\n",
    "    \"\"\"Loads the spec system prompt from the file system.\"\"\"\n",
    "    prompt_path = os.path.join(config.get(\"PROMPTS_DIR\"), \"spec_system.md\")\n",
    "    with open(prompt_path, 'r') as f:\n",
    "        spec_system_content = f.read()\n",
    "    return spec_system_content\n",
    "\n",
    "# Load the spec system prompt\n",
    "SPEC_SYSTEM_PROMPT = load_spec_system_prompt()\n",
    "print(f\"Loaded spec system prompt: {len(SPEC_SYSTEM_PROMPT)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Checkpoint Builder Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_BUILDER_USER_PROMPT = \"\"\"<user_prompt>\n",
    "    <input_text>\n",
    "        - As a Spec Prompt Engineer and Checkpoint Execution Strategist, please analyze the provided inputs and generate a comprehensive specification document.\n",
    "        - Ensure you follow and complete all phases of the `<internal_workflow>` to produce the full specification.\n",
    "        - Take all the time needed to generate the best possible specification document; there is no rush.\n",
    "    </input_text>\n",
    "    <app_purpose_input>\n",
    "        {app_purpose_content_placeholder}\n",
    "    </app_purpose_input>\n",
    "    <high_level_objective>\n",
    "        {user_requests_content_placeholder}\n",
    "    </user_requests_input>\n",
    "    <file_tree_input>\n",
    "        {file_tree_content_placeholder}\n",
    "    </file_tree_input>\n",
    "    <all_file_analysis>\n",
    "        {relevant_files_content_placeholder}\n",
    "    </all_file_analysis>\n",
    "    <previous_request_checkpoint_optional>\n",
    "        {previous_request_checkpoint_optional_placeholder}\n",
    "    </previous_request_checkpoint_optional>\n",
    "    <current_request_checkpoint_shell>\n",
    "        {request_checkpoint_shell}\n",
    "    </current_request_checkpoint_shell>\n",
    "    <final_thoughts>Take a deep breath, and please begin constructing the specification prompt based on these inputs.</final_thoughts>\n",
    "</user_prompt>\n",
    "\"\"\"\n",
    "\n",
    "def format_checkpoint_spec_user_prompt(\n",
    "    app_purpose_content: str,\n",
    "    user_requests_content: str,\n",
    "    file_tree_content: str,\n",
    "    relevant_files_content: str,\n",
    "    request_checkpoint_shell: str,\n",
    "    previous_request_checkpoint_optional: str = None\n",
    ") -> str:\n",
    "    \"\"\"Formats the user prompt for the checkpoint builder.\"\"\"\n",
    "    user_prompt = CHECKPOINT_BUILDER_USER_PROMPT.format(\n",
    "        app_purpose_content_placeholder=app_purpose_content.strip(),\n",
    "        user_requests_content_placeholder=user_requests_content.strip(),\n",
    "        file_tree_content_placeholder=file_tree_content.strip(),\n",
    "        relevant_files_content_placeholder=relevant_files_content.strip(),\n",
    "        request_checkpoint_shell=request_checkpoint_shell.strip(),\n",
    "        previous_request_checkpoint_optional_placeholder=previous_request_checkpoint_optional\n",
    "    )\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Detailed Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model\n",
    "model_name = \"gemini-2.5-pro-preview-05-06\"\n",
    "@google.call(\n",
    "    model_name,\n",
    "    response_model=RequestCheckpoint,\n",
    "    call_params={\n",
    "        \"config\": types.GenerateContentConfig(\n",
    "            thinking_config=types.ThinkingConfig(\n",
    "                thinking_budget=config.get(\"THINKING_BUDGET\")\n",
    "            ),\n",
    "            temperature=config.get(\"TEMPERATURE\")\n",
    "        )\n",
    "    },\n",
    ")\n",
    "def generate_checkpoint(system_prompt: str, user_prompt: str) -> str:\n",
    "    return [Messages.System(system_prompt), Messages.User(user_prompt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 7 checkpoints...\n",
      "\n",
      "Processing checkpoint 1/7: Implement New Server Actions for Context Bundles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ford/anaconda3/envs/promo/lib/python3.12/site-packages/google/genai/_common.py:240: UserWarning: null is not a valid Type\n",
      "  warnings.warn(f\"{value} is not a valid {cls.__name__}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved to: interim_data/generated_checkpoints/context_manager_update_frontend/checkpoint_01.json\n",
      "Processing checkpoint 2/7: Refactor Legacy `resolveContextSelectionAction`\n",
      "  ✓ Saved to: interim_data/generated_checkpoints/context_manager_update_frontend/checkpoint_02.json\n",
      "Processing checkpoint 3/7: Implement Frontend State Management for Active Context Bundle\n",
      "  ✓ Saved to: interim_data/generated_checkpoints/context_manager_update_frontend/checkpoint_03.json\n",
      "Processing checkpoint 4/7: Integrate Context Bundle ID into Frontend Operations and State\n",
      "  ✓ Saved to: interim_data/generated_checkpoints/context_manager_update_frontend/checkpoint_04.json\n",
      "Processing checkpoint 5/7: Implement Frontend Display of Context Bundle Snapshots on Cards\n",
      "  ✓ Saved to: interim_data/generated_checkpoints/context_manager_update_frontend/checkpoint_05.json\n",
      "Processing checkpoint 6/7: Update Documentation to Reflect Server-Action Architecture\n",
      "  ✓ Saved to: interim_data/generated_checkpoints/context_manager_update_frontend/checkpoint_06.json\n",
      "Processing checkpoint 7/7: End-to-End Testing and Quality Assurance\n",
      "  ✓ Saved to: interim_data/generated_checkpoints/context_manager_update_frontend/checkpoint_07.json\n",
      "\n",
      "Generated 7 detailed checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Process each checkpoint\n",
    "previous_request_checkpoint = None\n",
    "generated_checkpoints = []\n",
    "\n",
    "# Allow limiting the number of checkpoints to process (for testing)\n",
    "MAX_CHECKPOINTS_TO_PROCESS = None  # Set to a number to limit, or None to process all\n",
    "START_FROM_CHECKPOINT = None  # Set to a number to start from that checkpoint (1-based), or None to start from beginning\n",
    "\n",
    "checkpoints_to_process = request_checkpoint_outline.request_checkpoints\n",
    "if MAX_CHECKPOINTS_TO_PROCESS:\n",
    "    checkpoints_to_process = checkpoints_to_process[:MAX_CHECKPOINTS_TO_PROCESS]\n",
    "\n",
    "# Load previous checkpoint if starting from middle\n",
    "if START_FROM_CHECKPOINT and START_FROM_CHECKPOINT > 1:\n",
    "    checkpoint_output_dir = os.path.join(config.get(\"INTERIM_DATA_DIR\"), \"generated_checkpoints\", INPUT_NAME)\n",
    "    prev_checkpoint_path = os.path.join(checkpoint_output_dir, f\"checkpoint_{START_FROM_CHECKPOINT-1:02d}.json\")\n",
    "    if os.path.exists(prev_checkpoint_path):\n",
    "        with open(prev_checkpoint_path, \"r\") as f:\n",
    "            previous_request_checkpoint = f.read()\n",
    "        # Load existing checkpoints\n",
    "        generated_checkpoints = []\n",
    "        for i in range(1, START_FROM_CHECKPOINT):\n",
    "            checkpoint_path = os.path.join(checkpoint_output_dir, f\"checkpoint_{i:02d}.json\")\n",
    "            with open(checkpoint_path, \"r\") as f:\n",
    "                generated_checkpoints.append(RequestCheckpoint.model_validate_json(f.read()))\n",
    "        print(f\"Loaded {len(generated_checkpoints)} previous checkpoints\")\n",
    "        # Slice remaining checkpoints\n",
    "        checkpoints_to_process = checkpoints_to_process[START_FROM_CHECKPOINT-1:]\n",
    "    else:\n",
    "        print(f\"Warning: Could not find previous checkpoint at {prev_checkpoint_path}\")\n",
    "        print(\"Starting from beginning...\")\n",
    "        START_FROM_CHECKPOINT = None\n",
    "\n",
    "print(f\"Processing {len(checkpoints_to_process)} checkpoints...\\n\")\n",
    "\n",
    "for i, checkpoint_outline in enumerate(checkpoints_to_process, start=START_FROM_CHECKPOINT or 1):\n",
    "    print(f\"Processing checkpoint {i}/{len(request_checkpoint_outline.request_checkpoints)}: {checkpoint_outline.title}\")\n",
    "    \n",
    "    # Format the user prompt\n",
    "    checkpoint_outline_user_prompt = format_checkpoint_spec_user_prompt(\n",
    "        GLOBAL_CODEBASE_PURPOSE, \n",
    "        user_instructions, \n",
    "        file_tree, \n",
    "        '\\n'.join([analysis.model_dump_json(indent=2) for analysis in all_analyses_with_content]),\n",
    "        checkpoint_outline.model_dump_json(indent=2),\n",
    "        previous_request_checkpoint\n",
    "    )\n",
    "    \n",
    "    # Generate the detailed checkpoint\n",
    "    generated_checkpoint = generate_checkpoint(SPEC_SYSTEM_PROMPT, checkpoint_outline_user_prompt)\n",
    "    generated_checkpoints.append(generated_checkpoint)\n",
    "    \n",
    "    # Save individual checkpoint\n",
    "    checkpoint_output_dir = os.path.join(config.get(\"INTERIM_DATA_DIR\"), \"generated_checkpoints\", INPUT_NAME)\n",
    "    os.makedirs(checkpoint_output_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint_filename = f\"checkpoint_{i:02d}.json\"\n",
    "    checkpoint_path = os.path.join(checkpoint_output_dir, checkpoint_filename)\n",
    "    with open(checkpoint_path, \"w\") as f:\n",
    "        f.write(generated_checkpoint.model_dump_json(indent=2))\n",
    "    \n",
    "    print(f\"  ✓ Saved to: {checkpoint_path}\")\n",
    "    \n",
    "    # Update previous checkpoint for next iteration\n",
    "    previous_request_checkpoint = generated_checkpoint.model_dump_json(indent=2)\n",
    "    \n",
    "    # Optional: Add a break here if you want to process just one checkpoint for testing\n",
    "    # break\n",
    "\n",
    "print(f\"\\nGenerated {len(generated_checkpoints)} detailed checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Phase 3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated_checkpoints to interim_data/phase3/generated_checkpoints.json\n",
      "Saved phase3_summary to interim_data/phase3/phase3_summary.json\n",
      "\n",
      "Phase 3 Summary:\n",
      "==================================================\n",
      "\n",
      "Checkpoint 1: Implement New Server Actions for Context Bundles\n",
      "  - Tranches: 2\n",
      "  - Total tasks: 9\n",
      "  - Beginning files: 2\n",
      "  - Ending files: 4\n",
      "\n",
      "Checkpoint 2: Refactor Legacy `resolveContextSelectionAction`\n",
      "  - Tranches: 1\n",
      "  - Total tasks: 1\n",
      "  - Beginning files: 5\n",
      "  - Ending files: 5\n",
      "\n",
      "Checkpoint 3: Implement Frontend State Management for Active Context Bundle\n",
      "  - Tranches: 1\n",
      "  - Total tasks: 1\n",
      "  - Beginning files: 5\n",
      "  - Ending files: 6\n",
      "\n",
      "Checkpoint 4: Integrate Context Bundle ID into Frontend Operations and State\n",
      "  - Tranches: 2\n",
      "  - Total tasks: 2\n",
      "  - Beginning files: 6\n",
      "  - Ending files: 8\n",
      "\n",
      "Checkpoint 5: Implement Frontend Display of Context Bundle Snapshots on Cards\n",
      "  - Tranches: 1\n",
      "  - Total tasks: 1\n",
      "  - Beginning files: 8\n",
      "  - Ending files: 10\n",
      "\n",
      "Checkpoint 6: Update Documentation to Reflect Server-Action Architecture\n",
      "  - Tranches: 6\n",
      "  - Total tasks: 10\n",
      "  - Beginning files: 10\n",
      "  - Ending files: 18\n",
      "\n",
      "Checkpoint 7: End-to-End Testing and Quality Assurance\n",
      "  - Tranches: 4\n",
      "  - Total tasks: 8\n",
      "  - Beginning files: 18\n",
      "  - Ending files: 19\n",
      "\n",
      "Phase 3 completed successfully!\n",
      "Results saved to: interim_data/phase3/\n"
     ]
    }
   ],
   "source": [
    "# Save all generated checkpoints\n",
    "save_interim_data(config.get(\"INTERIM_DATA_DIR\"), generated_checkpoints, \"generated_checkpoints\", \"phase3\")\n",
    "\n",
    "# Create a summary of the generated checkpoints\n",
    "phase3_summary = {\n",
    "    \"checkpoints_generated\": len(generated_checkpoints),\n",
    "    \"checkpoint_details\": []\n",
    "}\n",
    "\n",
    "for checkpoint in generated_checkpoints:\n",
    "    checkpoint_detail = {\n",
    "        \"order\": checkpoint.order,\n",
    "        \"title\": checkpoint.title,\n",
    "        \"tranches_count\": len(checkpoint.execution_spec.task_tranches),\n",
    "        \"total_tasks\": sum(len(tranche.low_level_tasks) for tranche in checkpoint.execution_spec.task_tranches),\n",
    "        \"beginning_files\": len(checkpoint.request_checkpoint_specific_file_context.beginning_files_for_request_checkpoint),\n",
    "        \"ending_files\": len(checkpoint.request_checkpoint_specific_file_context.ending_files_for_request_checkpoint)\n",
    "    }\n",
    "    phase3_summary[\"checkpoint_details\"].append(checkpoint_detail)\n",
    "\n",
    "save_interim_data(config.get(\"INTERIM_DATA_DIR\"), phase3_summary, \"phase3_summary\", \"phase3\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nPhase 3 Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for detail in phase3_summary[\"checkpoint_details\"]:\n",
    "    print(f\"\\nCheckpoint {detail['order']}: {detail['title']}\")\n",
    "    print(f\"  - Tranches: {detail['tranches_count']}\")\n",
    "    print(f\"  - Total tasks: {detail['total_tasks']}\")\n",
    "    print(f\"  - Beginning files: {detail['beginning_files']}\")\n",
    "    print(f\"  - Ending files: {detail['ending_files']}\")\n",
    "\n",
    "print(f\"\\nPhase 3 completed successfully!\")\n",
    "print(f\"Results saved to: {config.get('INTERIM_DATA_DIR')}/phase3/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
