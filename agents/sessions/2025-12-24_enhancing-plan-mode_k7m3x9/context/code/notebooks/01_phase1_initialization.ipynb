{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Initialization and Understanding\n",
    "\n",
    "This notebook handles:\n",
    "1. Loading user inputs from file\n",
    "2. Performing initial file analysis on all relevant files\n",
    "3. Generating draft high-level request objective\n",
    "4. Saving results for next phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shared utilities\n",
    "import sys\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "from mirascope.core import google\n",
    "from mirascope import Messages\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict\n",
    "import asyncio\n",
    "\n",
    "from utils.interim_data_management import save_interim_data\n",
    "from utils.format_files_dict_to_xml import format_files_dict_to_xml\n",
    "from utils.config import update_config, get_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 specific configuration\n",
    "INPUT_NAME = \"context_manager_update_frontend\"  # Change this to process different inputs\n",
    "GLOBAL_CODEBASE_PURPOSE = \"AI-powered content creation and optimization platform for social media thought leadership. Transform raw ideas and long-form content into high-performing social media posts through intelligent content processing, hook optimization, and multi-format post generation\"\n",
    "\n",
    "# Update global config if needed\n",
    "update_config(DEBUG=False)\n",
    "\n",
    "config = get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Receive User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 55 files from context_manager_update_frontend\n",
      "User instructions length: 8679 characters\n"
     ]
    }
   ],
   "source": [
    "from utils.extract_xml_tags import extract_xml_content\n",
    "from utils.build_files_dict import build_files_dict\n",
    "\n",
    "filename = f\"../inputs/{INPUT_NAME}.txt\"\n",
    "\n",
    "# Tags to extract\n",
    "tags_to_extract = [\"file_map\", \"file_contents\", \"user_instructions\", \"Referenced APIs\"]\n",
    "\n",
    "# Extract content\n",
    "extracted_data = extract_xml_content(filename, tags_to_extract)\n",
    "\n",
    "# Return the three strings as a tuple\n",
    "file_tree = extracted_data.get(\"file_map\", \"\")\n",
    "file_contents = extracted_data.get(\"file_contents\", \"\")\n",
    "user_instructions = extracted_data.get(\"user_instructions\", \"\")\n",
    "referenced_apis = extracted_data.get(\"Referenced APIs\", \"\")\n",
    "files_dict = build_files_dict(file_contents)\n",
    "\n",
    "print(f\"Loaded {len(files_dict)} files from {INPUT_NAME}\")\n",
    "print(f\"User instructions length: {len(user_instructions)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw_inputs to interim_data/phase1/raw_inputs.json\n",
      "Saved files_dict to interim_data/phase1/files_dict.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'interim_data/phase1/files_dict.json'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save raw inputs for reference\n",
    "raw_inputs = {\n",
    "    \"input_name\": INPUT_NAME,\n",
    "    \"file_tree\": file_tree,\n",
    "    \"user_instructions\": user_instructions,\n",
    "    \"referenced_apis\": referenced_apis,\n",
    "    \"global_codebase_purpose\": GLOBAL_CODEBASE_PURPOSE,\n",
    "    \"files_count\": len(files_dict)\n",
    "}\n",
    "\n",
    "save_interim_data(config.get(\"INTERIM_DATA_DIR\"), raw_inputs, \"raw_inputs\", \"phase1\")\n",
    "save_interim_data(config.get(\"INTERIM_DATA_DIR\"), files_dict, \"files_dict\", \"phase1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Define Models and Prompts for File Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialFileAnalysisItem(BaseModel):\n",
    "    file_path: str\n",
    "    overall_relevance_to_request_objective: str\n",
    "    initial_content_summary_purpose: str\n",
    "\n",
    "class InitialFileAnalysisItemWithContent(InitialFileAnalysisItem):\n",
    "    file_contents: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_ANALYSIS_SYSTEM_PROMPT = \"\"\"<system_context>\n",
    "You are an expert code analyst. \n",
    "Your task is to understand a specific code file within the context of a larger request and a user's request. \n",
    "You need to provide a concise analysis focusing on two key aspects: \n",
    "- Its overall relevance to the request's goal\n",
    "- A summary of its current content/purpose.\n",
    "</system_context>\n",
    "\n",
    "<input_data>\n",
    "    <overall_codebase_purpose>\n",
    "    {overall_codebase_purpose_placeholder}\n",
    "    </overall_codebase_purpose>\n",
    "\n",
    "    <user_request>\n",
    "    {user_request_placeholder}\n",
    "    </user_request>\n",
    "\n",
    "    <file_tree>\n",
    "    {file_tree_placeholder}\n",
    "    <file_tree>\n",
    "\n",
    "    <all_other_relevant_files_summary>\n",
    "    {summary_of_other_files_placeholder}\n",
    "    </all_other_relevant_files_summary>\n",
    "</input_data>\n",
    "\n",
    "<output_instructions>\n",
    "Based on all the provided input data, generate the following for the <file_to_analyze>:\n",
    "\n",
    "1.  **Overall Relevance to Request Objective:**\n",
    "    *   Directly state how this file relates to the request's goal. Be brief and specific (e.g., \"specifies Supabase request being used\").\n",
    "    *   Note if it is a core component, utility, config, UI, data model, etc., in relation to the request.\n",
    "    *   If relevant, mention any transitional or legacy role (e.g., \"still holds request ID useful for migration\").\n",
    "    *   Keep this concise (2-4 sentences).\n",
    "\n",
    "2.  **Initial Content Summary/Purpose:**\n",
    "    *   Succinctly state the file's main purpose or function as it currently exists (e.g., \"configures the Supabase request ID\").\n",
    "    *   Focus on what it does now, not future changes.\n",
    "    *   Be as direct and concise as possible.\n",
    "    *   Keep this concise (2-4 sentences).\n",
    "</output_instructions>\n",
    "\"\"\"\n",
    "\n",
    "FILE_ANALYSIS_USER_PROMPT = \"\"\"<user_request>\n",
    "    <input_text>Please analyze the following file and provide a concise summary of its purpose and functionality.</input_text>\n",
    "    <file_to_analyze>\n",
    "        {file_content_placeholder}\n",
    "    </file_to_analyze>\n",
    "</user_request>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@google.call(\n",
    "    \"gemini-2.0-flash\",\n",
    "    response_model=InitialFileAnalysisItem,\n",
    "    call_params={\n",
    "        \"config\": types.GenerateContentConfig(temperature=0.2)\n",
    "    },\n",
    ")\n",
    "def async_file_analysis(system_prompt: str, user_prompt: str) -> str:\n",
    "    return [Messages.System(system_prompt), Messages.User(user_prompt)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.3: Perform Initial File Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting file analyses...\n",
      "Completed 55 file analyses.\n"
     ]
    }
   ],
   "source": [
    "async def generate_all_file_analyses(\n",
    "    files_dict: Dict[str, str],\n",
    "    overall_codebase_purpose: str,\n",
    "    user_request_text: str,\n",
    "    file_tree_text: str,\n",
    "    file_analysis_system_template: str,\n",
    "    file_analysis_user_template: str\n",
    ") -> List['InitialFileAnalysisItem']:\n",
    "    \"\"\"\n",
    "    Generates file analysis items for all files in the files_dict asynchronously.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "\n",
    "    for file_path, current_file_content in files_dict.items():\n",
    "        # 1. Create a dictionary containing all other files and their content\n",
    "        other_files_dict = {\n",
    "            fp: content for fp, content in files_dict.items() if fp != file_path\n",
    "        }\n",
    "\n",
    "        # 2. Format the dictionary of other files into an XML string\n",
    "        other_files_summary_xml = format_files_dict_to_xml(other_files_dict)\n",
    "\n",
    "        # 3. Populate the system prompt\n",
    "        system_prompt = file_analysis_system_template.replace(\n",
    "            \"{overall_codebase_purpose_placeholder}\", overall_codebase_purpose\n",
    "        ).replace(\n",
    "            \"{user_request_placeholder}\", user_request_text\n",
    "        ).replace(\n",
    "            \"{file_tree_placeholder}\", file_tree_text\n",
    "        ).replace(\n",
    "            \"{summary_of_other_files_placeholder}\", other_files_summary_xml\n",
    "        )\n",
    "\n",
    "        # 4. Populate the user prompt\n",
    "        prompt_file_content_with_path = f\"File Path: {file_path}\\n\\n{current_file_content}\"\n",
    "        \n",
    "        user_prompt = file_analysis_user_template.replace(\n",
    "            \"{file_content_placeholder}\", prompt_file_content_with_path\n",
    "        )\n",
    "        \n",
    "        task = asyncio.to_thread(async_file_analysis, system_prompt, user_prompt)\n",
    "        tasks.append(task)\n",
    "\n",
    "    # Run all analysis tasks concurrently\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Starting file analyses...\")\n",
    "all_analyses = await generate_all_file_analyses(\n",
    "    files_dict=files_dict,\n",
    "    overall_codebase_purpose=GLOBAL_CODEBASE_PURPOSE,\n",
    "    user_request_text=user_instructions,\n",
    "    file_tree_text=file_tree,\n",
    "    file_analysis_system_template=FILE_ANALYSIS_SYSTEM_PROMPT,\n",
    "    file_analysis_user_template=FILE_ANALYSIS_USER_PROMPT\n",
    ")\n",
    "print(f\"Completed {len(all_analyses)} file analyses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file_analyses to interim_data/phase1/file_analyses.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'interim_data/phase1/file_analyses.json'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add file contents to analysis items\n",
    "all_analyses_with_content = []\n",
    "for analysis_item in all_analyses:\n",
    "    file_contents = files_dict.get(analysis_item.file_path, \"\")\n",
    "    item_with_content = InitialFileAnalysisItemWithContent(\n",
    "        **analysis_item.__dict__,\n",
    "        file_contents=file_contents\n",
    "    )\n",
    "    all_analyses_with_content.append(item_with_content)\n",
    "\n",
    "# Save file analyses\n",
    "save_interim_data(config.get(\"INTERIM_DATA_DIR\"), all_analyses_with_content, \"file_analyses\", \"phase1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4: Generate Draft High-Level Request Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNDERSTAND_USER_REQUEST_SYSTEM_PROMPT = \"\"\"<system_context>\n",
    "You are an expert technical writer and request planner. \n",
    "Your primary task is to synthesize a user's request and an initial analysis of relevant code files into a clear and comprehensive high-level request objective. \n",
    "This objective is CRITICAL as it ensures both the system and the user are perfectly aligned on the overall goal before proceeding with detailed planning. \n",
    "It will inform the entire generation pipeline.\n",
    "</system_context>\n",
    "\n",
    "<input_data>\n",
    "    <user_original_request>\n",
    "    {user_original_request_placeholder}\n",
    "    </user_original_request>\n",
    "\n",
    "    <overall_codebase_purpose>\n",
    "    {overall_codebase_purpose_placeholder}\n",
    "    </overall_codebase_purpose>\n",
    "\n",
    "    <file_tree>\n",
    "    {file_tree_placeholder}\n",
    "    </file_tree>\n",
    "\n",
    "    <initial_file_analysis_summary>\n",
    "    {initial_file_analysis_summary_placeholder}\n",
    "    </initial_file_analysis_summary>\n",
    "</input_data>\n",
    "\n",
    "<output_instructions>\n",
    "Based on the <user_original_request>, the <overall_codebase_purpose>, and the <initial_file_analysis_summary>, please formulate a high-level request objective.\n",
    "\n",
    "This objective MUST:\n",
    "1.  Clearly and unambiguously state what will be achieved, built, or changed upon completion of the entire request/task.\n",
    "2.  Be comprehensive enough to capture the full scope of the user's intent, typically within 2-4 sentences. The aim is complete understanding, not just brevity.\n",
    "3.  Accurately reflect the core intent of the user's request, grounded in the context of the existing codebase and relevant files.\n",
    "4.  Focus on the \"what\" (the end state or primary deliverable) and avoid detailing specific \"how-to\" implementation steps or sub-tasks at this stage.\n",
    "\n",
    "The goal is to create a statement that, if confirmed by the user, signifies complete agreement on the request's ultimate aim.\n",
    "\n",
    "Output only the high-level request objective.\n",
    "</output_instructions>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverallRequestObjectiveResponse(BaseModel):\n",
    "    overall_request_objective: str\n",
    "\n",
    "@google.call(\n",
    "    \"gemini-2.5-pro-preview-05-06\",\n",
    "    response_model=OverallRequestObjectiveResponse,\n",
    "    call_params={\n",
    "        \"config\": types.GenerateContentConfig(\n",
    "            thinking_config=types.ThinkingConfig(thinking_budget=2048), \n",
    "            temperature=0.5\n",
    "        )\n",
    "    },\n",
    ")\n",
    "def generate_overall_request_objective(system_prompt: str) -> str:\n",
    "    return [Messages.System(system_prompt), Messages.User(\"Please generate the overall request objective.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format and generate objective\n",
    "def format_overall_summary_system_prompt(\n",
    "    user_original_request: str,\n",
    "    overall_codebase_purpose: str,\n",
    "    file_tree: str,\n",
    "    initial_file_analysis_summary: str\n",
    ") -> str:\n",
    "    system_prompt = UNDERSTAND_USER_REQUEST_SYSTEM_PROMPT.replace(\n",
    "        \"{user_original_request_placeholder}\", user_original_request.strip()\n",
    "    ).replace(\n",
    "        \"{overall_codebase_purpose_placeholder}\", overall_codebase_purpose.strip()\n",
    "    ).replace(\n",
    "        \"{file_tree_placeholder}\", file_tree\n",
    "    ).replace(\n",
    "        \"{initial_file_analysis_summary_placeholder}\", initial_file_analysis_summary.strip()\n",
    "    )\n",
    "    return system_prompt\n",
    "\n",
    "overall_summary_system_prompt = format_overall_summary_system_prompt(\n",
    "    user_instructions, \n",
    "    GLOBAL_CODEBASE_PURPOSE, \n",
    "    file_tree, \n",
    "    '\\n'.join([analysis.model_dump_json(indent=2) for analysis in all_analyses_with_content])\n",
    ")\n",
    "\n",
    "# Uncomment to generate the objective\n",
    "# overall_request_objective = generate_overall_request_objective(overall_summary_system_prompt).overall_request_objective\n",
    "# print(f\"Generated objective: {overall_request_objective}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Phase 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved phase1_summary to interim_data/phase1/phase1_summary.json\n",
      "\n",
      "Phase 1 completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save all Phase 1 results\n",
    "phase1_summary = {\n",
    "    \"input_name\": INPUT_NAME,\n",
    "    \"files_analyzed\": len(all_analyses_with_content),\n",
    "    \"overall_codebase_purpose\": GLOBAL_CODEBASE_PURPOSE,\n",
    "    \"user_instructions_length\": len(user_instructions),\n",
    "    # \"overall_request_objective\": overall_request_objective  # Uncomment when generated\n",
    "}\n",
    "\n",
    "save_interim_data(config.get(\"INTERIM_DATA_DIR\"), phase1_summary, \"phase1_summary\", \"phase1\")\n",
    "print(\"\\nPhase 1 completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
